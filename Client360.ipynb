{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ccf10b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jai Shree Ram\n"
     ]
    }
   ],
   "source": [
    "print(\"Jai Shree Ram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bee609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "C86 Client 360 - RBC\n",
    "Author: Maharaj Aryan Kumar\n",
    "Date: 09/11/2025\n",
    "Supervisor: AP\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2d05d",
   "metadata": {},
   "source": [
    "Screenshot 1\n",
    "Detects DEV/PROD, sets base folders, creates today’s log/listing files, and redirects SAS logging/printing to those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad634a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>host</th>\n",
       "      <th>env</th>\n",
       "      <th>regpath</th>\n",
       "      <th>logpath</th>\n",
       "      <th>outpath</th>\n",
       "      <th>logfile</th>\n",
       "      <th>lstfile</th>\n",
       "      <th>run_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maharaj</td>\n",
       "      <td>aryans-macbook-pro.local</td>\n",
       "      <td>DEV</td>\n",
       "      <td>/Users/maharaj/Documents/C86Client360_09112025...</td>\n",
       "      <td>/Users/maharaj/Documents/C86Client360_09112025...</td>\n",
       "      <td>/Users/maharaj/Documents/C86Client360_09112025...</td>\n",
       "      <td>/Users/maharaj/Documents/C86Client360_09112025...</td>\n",
       "      <td>/Users/maharaj/Documents/C86Client360_09112025...</td>\n",
       "      <td>2025-09-10 09:47:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user                      host  env  \\\n",
       "0  maharaj  aryans-macbook-pro.local  DEV   \n",
       "\n",
       "                                             regpath  \\\n",
       "0  /Users/maharaj/Documents/C86Client360_09112025...   \n",
       "\n",
       "                                             logpath  \\\n",
       "0  /Users/maharaj/Documents/C86Client360_09112025...   \n",
       "\n",
       "                                             outpath  \\\n",
       "0  /Users/maharaj/Documents/C86Client360_09112025...   \n",
       "\n",
       "                                             logfile  \\\n",
       "0  /Users/maharaj/Documents/C86Client360_09112025...   \n",
       "\n",
       "                                             lstfile         run_datetime  \n",
       "0  /Users/maharaj/Documents/C86Client360_09112025...  2025-09-10 09:47:16  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# --- Base directory: where the script is located (or current working directory in notebooks) ---\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()   # fallback for Jupyter/interactive\n",
    "\n",
    "# --- Helper function ---\n",
    "def ensure_dir(path: Path):\n",
    "    \"\"\"Create folder if it does not exist.\"\"\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# 1) Detect environment: DEV or PROD\n",
    "user = (os.getenv(\"USER\") or os.getenv(\"USERNAME\") or \"\").strip()\n",
    "host = socket.gethostname().lower()\n",
    "env = \"PROD\" if (user[:1].upper() == \"U\" and host != \"usasst11\") else \"DEV\"\n",
    "\n",
    "# 2) Base path inside project folder\n",
    "regpath = BASE_DIR / (\"REG\" if env == \"PROD\" else \"REG_DEV\")\n",
    "\n",
    "# 3) Output folders\n",
    "logpath = ensure_dir(regpath / \"C86\" / \"log\" / \"product_appropriateness\" / \"client360\")\n",
    "outpath = ensure_dir(regpath / \"output\" / \"product_appropriateness\" / \"client360\")\n",
    "\n",
    "# 4) Daily filenames (yyyymmdd)\n",
    "ymd = datetime.now().strftime(\"%Y%m%d\")\n",
    "logfile = logpath / f\"C86_pa_client360_{ymd}.log\"\n",
    "lstfile = logpath / f\"C86_pa_client360_{ymd}.lst\"\n",
    "\n",
    "# 5) Logging setup\n",
    "logging.basicConfig(\n",
    "    filename=str(logfile),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "logging.info(\"Starting run...\")\n",
    "logging.info(\"User=%s Host=%s Env=%s\", user, host, env)\n",
    "logging.info(\"regpath=%s\", str(regpath))\n",
    "logging.info(\"logpath=%s\", str(logpath))\n",
    "logging.info(\"outpath=%s\", str(outpath))\n",
    "\n",
    "# Also mimic SAS listing output\n",
    "with open(lstfile, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\">>> Starting Running Time - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Environment: {env}\\n\")\n",
    "    f.write(f\"User: {user}  Host: {host}\\n\")\n",
    "    f.write(f\"regpath: {regpath}\\n\\n\")\n",
    "\n",
    "# 6) Capture run configuration\n",
    "run_config = pd.DataFrame([{\n",
    "    \"user\": user,\n",
    "    \"host\": host,\n",
    "    \"env\": env,\n",
    "    \"regpath\": str(regpath),\n",
    "    \"logpath\": str(logpath),\n",
    "    \"outpath\": str(outpath),\n",
    "    \"logfile\": str(logfile),\n",
    "    \"lstfile\": str(lstfile),\n",
    "    \"run_datetime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}])\n",
    "\n",
    "run_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40c4d9",
   "metadata": {},
   "source": [
    "Screenshot 2\n",
    "Initial-run switch + date window\n",
    "\n",
    "Checks: \n",
    "\n",
    "client360_autocomplete.sas7bdat already exists in the output folder.\n",
    "\n",
    "If it exists → marks this as not an initial run, and renames that file as a timestamped backup.\n",
    "\n",
    "If it doesn’t → treats this as the initial run.\n",
    "\n",
    "Sets up simple dates: today, a week window (start/end), and runday.\n",
    "\n",
    "For the very first run, it uses fixed launch dates; for later runs, it uses the current week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a12ca55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run dates:\n",
      " ini_run   wk_start wk_start_min14     wk_end   runday     tday                                                                                         outpath                                                                                            runday_folder\n",
      "      Y 2023-05-07     2023-04-23 2025-09-14 20250910 20250910 /Users/maharaj/Documents/C86Client360_09112025/REG_DEV/output/product_appropriateness/client360 /Users/maharaj/Documents/C86Client360_09112025/REG_DEV/output/product_appropriateness/client360/20250910\n"
     ]
    }
   ],
   "source": [
    "# 1) Check for existing dataset to decide initial run\n",
    "autocomplete_file = outpath / \"pa_client360_autocomplete.sas7bdat\"\n",
    "if autocomplete_file.exists():\n",
    "    ini_run = \"N\"\n",
    "    backup_name = f\"pa_client360_autocomplete_backup_{datetime.now().strftime('%Y%m%d%H%M%S')}.sas7bdat\"\n",
    "    try:\n",
    "        autocomplete_file.rename(outpath / backup_name)\n",
    "    except Exception as e:\n",
    "        logging.warning(\"Could not rename existing autocomplete file: %s\", e)\n",
    "else:\n",
    "    ini_run = \"Y\"\n",
    "\n",
    "# 2) Today's date and launch reference dates (match SAS literals)\n",
    "from datetime import date, timedelta\n",
    "\n",
    "tday = date.today()\n",
    "launch_dt = date(2023, 5, 7)         # '07MAY2023'd\n",
    "launch_dt_min14 = date(2023, 4, 23)  # '23APR2023'd\n",
    "\n",
    "# 3) Week window (Monday..Sunday). Adjust if needed.\n",
    "week_start = tday - timedelta(days=tday.weekday())\n",
    "week_end = week_start + timedelta(days=6)\n",
    "\n",
    "# 4) Choose window based on initial run\n",
    "if ini_run == \"Y\":\n",
    "    wk_start = launch_dt\n",
    "    wk_start_min14 = launch_dt_min14\n",
    "    wk_end = week_end\n",
    "else:\n",
    "    wk_start = week_start\n",
    "    wk_start_min14 = week_start - timedelta(days=14)\n",
    "    wk_end = week_end\n",
    "\n",
    "# 5) Handy strings like SAS yyyymmdd\n",
    "runday = tday.strftime(\"%Y%m%d\")\n",
    "tday_str = tday.strftime(\"%Y%m%d\")\n",
    "wk_start_str = wk_start.strftime(\"%Y-%m-%d\")\n",
    "wk_start_min14_str = wk_start_min14.strftime(\"%Y-%m-%d\")\n",
    "wk_end_str = wk_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 6) Create daily output folder (mimics libname dataout \"&outpath/&runday\")\n",
    "runday_folder = outpath / runday\n",
    "runday_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 7) Show results in a DataFrame\n",
    "run_dates = pd.DataFrame([{\n",
    "    \"ini_run\": ini_run,\n",
    "    \"wk_start\": wk_start_str,\n",
    "    \"wk_start_min14\": wk_start_min14_str,\n",
    "    \"wk_end\": wk_end_str,\n",
    "    \"runday\": runday,\n",
    "    \"tday\": tday_str,\n",
    "    \"outpath\": str(outpath),\n",
    "    \"runday_folder\": str(runday_folder),\n",
    "}])\n",
    "\n",
    "print(\"\\nRun dates:\\n\", run_dates.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a929d6",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "\n",
    "Screenshot 3 — Pull data from Tracking (Teradata) + simple aggregations\n",
    "\n",
    "What it does: Connects to Teradata, pulls recent \"Advice Tool\" events\n",
    "\n",
    "(EVNT_DT > wk_start - 90 days), then builds two beginner-friendly DataFrames:\n",
    "\n",
    "    1) tracking_tool_use_distinct: distinct (OPPOR_ID, ADVC_TOOL_NM)\n",
    "\n",
    "    2) tracking_count_tool_use_pre2: per OPPOR_ID, count of unique tools used\n",
    "\n",
    "Results are also saved as CSVs into the runday folder.\n",
    "\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415a1dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows pulled: 0\n",
      "Distinct pairs: 0\n",
      "OPPOR_ID tool-count rows: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "# Helper: get a Teradata connection using a small JSON file.\n",
    "def get_teradata_conn(config_path: str = \"TeradataConnection_T.json\"):\n",
    "    try:\n",
    "        import teradatasql  # pip install teradatasql\n",
    "    except Exception:\n",
    "        logging.warning(\"teradatasql not available; returning None. Install with: pip install teradatasql\")\n",
    "        return None\n",
    "\n",
    "    cfg = Path(config_path)\n",
    "    if not cfg.exists():\n",
    "        logging.warning(\"Config file %s not found. Skipping DB fetch.\", config_path)\n",
    "        return None\n",
    "\n",
    "    with open(cfg) as f:\n",
    "        creds = json.load(f)\n",
    "\n",
    "    try:\n",
    "        conn = teradatasql.connect(\n",
    "            host=creds[\"url\"],\n",
    "            user=creds[\"user\"],\n",
    "            password=creds[\"password\"],\n",
    "            logmech=\"LDAP\",\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logging.error(\"Teradata connection failed: %s\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Compute the 90-day lookback from wk_start (defined earlier) ---\n",
    "start_90 = wk_start - timedelta(days=90)\n",
    "start_literal = start_90.strftime(\"%Y-%m-%d\")  # DATE 'YYYY-MM-DD' in SQL\n",
    "\n",
    "# --- Columns we actually need ---\n",
    "cols = [\"OPPOR_ID\", \"ADVC_TOOL_NM\"]\n",
    "\n",
    "# --- Default empty frame so code runs even without DB access ---\n",
    "tracking_all = pd.DataFrame(columns=cols)\n",
    "\n",
    "conn = get_teradata_conn()\n",
    "if conn is not None:\n",
    "    try:\n",
    "        sql = f\"\"\"\n",
    "            SELECT OPPOR_ID,\n",
    "                   ADVC_TOOL_NM\n",
    "            FROM DDW01.EVNT_PROD_TRACK_LOG\n",
    "            WHERE advr_selt_typ = 'Advice Tool'\n",
    "              AND EVNT_DT > DATE '{start_literal}'\n",
    "        \"\"\"\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        rows = cur.fetchall()\n",
    "        colnames = [d[0] for d in cur.description]\n",
    "        tracking_all = pd.DataFrame(rows, columns=colnames).reindex(cols, axis=1)\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        logging.error(\"Query failed: %s\", e)\n",
    "\n",
    "# --- Clean & standardize ---\n",
    "if not tracking_all.empty:\n",
    "    tracking_all = tracking_all.dropna(subset=[\"OPPOR_ID\", \"ADVC_TOOL_NM\"]).copy()\n",
    "    tracking_all[\"ADVC_TOOL_NM\"] = tracking_all[\"ADVC_TOOL_NM\"].astype(str).str.upper()\n",
    "\n",
    "# --- 1) Distinct (OPPOR_ID, ADVC_TOOL_NM) ---\n",
    "tracking_tool_use_distinct = tracking_all.drop_duplicates(subset=[\"OPPOR_ID\", \"ADVC_TOOL_NM\"])\n",
    "\n",
    "# --- 2) Count of unique tools used per OPPOR_ID ---\n",
    "tracking_count_tool_use_pre2 = (\n",
    "    tracking_all\n",
    "    .groupby(\"OPPOR_ID\", dropna=False)[\"ADVC_TOOL_NM\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"count_unique_tool_used\")\n",
    "    .sort_values(\"count_unique_tool_used\", ascending=False)\n",
    ")\n",
    "\n",
    "# --- Save simple CSV outputs (mimics SAS datasets) ---\n",
    "tracking_all.to_csv(runday_folder / \"tracking_all.csv\", index=False)\n",
    "tracking_tool_use_distinct.to_csv(runday_folder / \"tracking_tool_use_distinct.csv\", index=False)\n",
    "tracking_count_tool_use_pre2.to_csv(runday_folder / \"tracking_count_tool_use_pre2.csv\", index=False)\n",
    "\n",
    "print(\"\\nRows pulled:\", len(tracking_all))\n",
    "print(\"Distinct pairs:\", len(tracking_tool_use_distinct))\n",
    "print(\"OPPOR_ID tool-count rows:\", len(tracking_count_tool_use_pre2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd900acf",
   "metadata": {},
   "source": [
    "Screenshot 4 — “Tool Used” flag from aggregate\n",
    "\n",
    "What it does (in short):\n",
    "Takes the aggregated table tracking_count_tool_use_pre2 (unique tool count per OPPOR_ID) and builds a light flag table:\n",
    "\n",
    "OPPOR_ID\n",
    "\n",
    "tool_used = 'Tool Used' if count_unique_tool_used > 0, otherwise blank.\n",
    "\n",
    "It also saves tracking_tool_use.csv to the runday folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14015a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking_tool_use rows: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Guard: make sure upstream DataFrame exists\n",
    "tracking_count_tool_use_pre2 = locals().get(\n",
    "    \"tracking_count_tool_use_pre2\",\n",
    "    pd.DataFrame(columns=[\"OPPOR_ID\", \"count_unique_tool_used\"])  # empty fallback\n",
    ")\n",
    "\n",
    "tracking_tool_use = (\n",
    "    tracking_count_tool_use_pre2[[\"OPPOR_ID\", \"count_unique_tool_used\"]].copy()\n",
    ")\n",
    "\n",
    "# Mark if tool was used (mimics SAS logic)\n",
    "tracking_tool_use[\"tool_used\"] = np.where(\n",
    "    tracking_tool_use[\"count_unique_tool_used\"] > 0,\n",
    "    \"Tool Used\",\n",
    "    None\n",
    ")\n",
    "\n",
    "# Keep SAS-like column order\n",
    "tracking_tool_use = tracking_tool_use[[\"OPPOR_ID\", \"tool_used\"]]\n",
    "\n",
    "# Save CSV\n",
    "tracking_tool_use.to_csv(runday_folder / \"tracking_tool_use.csv\", index=False)\n",
    "\n",
    "print(\"tracking_tool_use rows:\", len(tracking_tool_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe129762",
   "metadata": {},
   "source": [
    "Screenshot 5 — “c360_short + c360_detail_pre (employee attributes by snapshot date)”\n",
    "\n",
    "What it does (in short):\n",
    "\n",
    "Builds a snapshot view c360_short from ddw01.evnt_prod_oppor with:\n",
    "\n",
    "evnt_id, emp_id = CAST(rbc_oppor_own_id AS INTEGER), snap_dt = evnt_dt\n",
    "\n",
    "Filters rows between wk_start and wk_end and requires ids/dates to be not null.\n",
    "\n",
    "Creates c360_detail_pre by joining that snapshot to HR tables (ddw01.emp, ddw01.empl_reltn) where the snapshot date falls between captr_dt and chg_dt (SCD-style “valid on date” join).\n",
    "\n",
    "Saves both DataFrames as CSVs in the runday folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faa3456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "c360_short rows: 0  | c360_detail_pre rows: 0\n"
     ]
    }
   ],
   "source": [
    "wk_start_lit = wk_start.strftime(\"%Y-%m-%d\")\n",
    "wk_end_lit = wk_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "c360_short = pd.DataFrame(columns=[\"evnt_id\", \"emp_id\", \"snap_dt\"])  # default empty\n",
    "c360_detail_pre = pd.DataFrame()  # default empty\n",
    "\n",
    "conn = get_teradata_conn()\n",
    "if conn is not None:\n",
    "    try:\n",
    "        # 1) Pull c360_short\n",
    "        sql_short = f\"\"\"\n",
    "            SELECT evnt_id,\n",
    "                   CAST(rbc_oppor_own_id AS INTEGER) AS emp_id,\n",
    "                   evnt_dt AS snap_dt\n",
    "            FROM ddw01.evnt_prod_oppor\n",
    "            WHERE rbc_oppor_own_id IS NOT NULL\n",
    "              AND evnt_dt IS NOT NULL\n",
    "              AND evnt_id IS NOT NULL\n",
    "              AND evnt_dt BETWEEN DATE '{wk_start_lit}' AND DATE '{wk_end_lit}'\n",
    "        \"\"\"\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql_short)\n",
    "            rows = cur.fetchall()\n",
    "            cols = [d[0] for d in cur.description]\n",
    "            c360_short = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "        # 2) Pull detailed attributes\n",
    "        sql_detail = f\"\"\"\n",
    "            WITH c360_short AS (\n",
    "                SELECT evnt_id,\n",
    "                       CAST(rbc_oppor_own_id AS INTEGER) AS emp_id,\n",
    "                       evnt_dt AS snap_dt\n",
    "                FROM ddw01.evnt_prod_oppor\n",
    "                WHERE rbc_oppor_own_id IS NOT NULL\n",
    "                  AND evnt_dt IS NOT NULL\n",
    "                  AND evnt_id IS NOT NULL\n",
    "                  AND evnt_dt BETWEEN DATE '{wk_start_lit}' AND DATE '{wk_end_lit}'\n",
    "            )\n",
    "            SELECT\n",
    "                c360.evnt_id,\n",
    "                c360.evnt_dt,\n",
    "                c360.rbc_oppor_own_id,\n",
    "                emp.org_unt_no,\n",
    "                emp.hr_posn_titl_en,\n",
    "                emp.posn_strt_dt,\n",
    "                emp.posn_end_dt,\n",
    "                emp.occpt_job_cd\n",
    "            FROM ddw01.evnt_prod_oppor AS c360\n",
    "            LEFT JOIN (\n",
    "                SELECT c3.evnt_id,\n",
    "                       e1.org_unt_no,\n",
    "                       e1.hr_posn_titl_en,\n",
    "                       e2.posn_strt_dt,\n",
    "                       e2.posn_end_dt,\n",
    "                       e1.occpt_job_cd\n",
    "                FROM c360_short AS c3\n",
    "                INNER JOIN ddw01.emp AS e1\n",
    "                  ON e1.emp_id = c3.emp_id\n",
    "                 AND c3.snap_dt >= e1.captr_dt\n",
    "                 AND c3.snap_dt <  e1.chg_dt\n",
    "                INNER JOIN ddw01.empl_reltn AS e2\n",
    "                  ON e2.emp_id = c3.emp_id\n",
    "                 AND c3.snap_dt >= e2.captr_dt\n",
    "                 AND c3.snap_dt <  e2.chg_dt\n",
    "            ) AS emp\n",
    "              ON emp.evnt_id = c360.evnt_id\n",
    "            WHERE c360.evnt_id IS NOT NULL\n",
    "              AND c360.evnt_dt BETWEEN DATE '{wk_start_lit}' AND DATE '{wk_end_lit}'\n",
    "        \"\"\"\n",
    "        with conn.cursor() as cur2:\n",
    "            cur2.execute(sql_detail)\n",
    "            rows2 = cur2.fetchall()\n",
    "            cols2 = [d[0] for d in cur2.description]\n",
    "            c360_detail_pre = pd.DataFrame(rows2, columns=cols2)\n",
    "\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        logging.error(\"query failed: %s\", e)\n",
    "\n",
    "# Save outputs to runday folder\n",
    "c360_short.to_csv(runday_folder / \"c360_short.csv\", index=False)\n",
    "c360_detail_pre.to_csv(runday_folder / \"c360_detail_pre.csv\", index=False)\n",
    "\n",
    "print(f\"\\nc360_short rows: {len(c360_short)}  | c360_detail_pre rows: {len(c360_detail_pre)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465a3a2",
   "metadata": {},
   "source": [
    "Screenshot 6 — “Join tool flag, quick frequency, and stage mapping”\n",
    "\n",
    "What it does (in short):\n",
    "\n",
    "Left joins tracking_tool_use into c360_detail_pre by OPPOR_ID.\n",
    "\n",
    "Builds a clean label TOOL_USED = 'Tool Used' when there’s a match, else 'Tool Not Used'.\n",
    "\n",
    "Prints a PROC FREQ–style frequency table of lob (if that column exists), and saves it.\n",
    "\n",
    "Creates a Python dict version of the SAS proc format stage mapping (e.g., \"Discovery/Understand Needs\" → \"12.Discovery/Understand Needs\"). This is ready to apply once we know the exact stage column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0450062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "c360_detail rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Guards for upstream DataFrames\n",
    "if 'c360_detail_pre' not in globals():\n",
    "    c360_detail_pre = pd.DataFrame()\n",
    "if 'tracking_tool_use' not in globals():\n",
    "    tracking_tool_use = pd.DataFrame(columns=[\"OPPOR_ID\", \"tool_used\"])  # empty fallback\n",
    "\n",
    "c360_detail = c360_detail_pre.copy()\n",
    "\n",
    "# Align join key if needed\n",
    "if not c360_detail.empty:\n",
    "    if 'OPPOR_ID' not in c360_detail.columns and 'rbc_oppor_own_id' in c360_detail.columns:\n",
    "        c360_detail = c360_detail.rename(columns={\"rbc_oppor_own_id\": \"OPPOR_ID\"})\n",
    "\n",
    "# Join with tool use info\n",
    "if not c360_detail.empty and 'OPPOR_ID' in c360_detail.columns:\n",
    "    c360_detail = c360_detail.merge(tracking_tool_use, how='left', on='OPPOR_ID')\n",
    "else:\n",
    "    logging.info(\"OPPOR_ID not found in c360_detail_pre; skipping merge and setting tool_used to None.\")\n",
    "    c360_detail['tool_used'] = None\n",
    "\n",
    "# TOOL_USED label\n",
    "c360_detail['TOOL_USED'] = c360_detail['tool_used'].apply(\n",
    "    lambda v: 'Tool Used' if pd.notnull(v) and str(v).strip() != '' else 'Tool Not Used'\n",
    ")\n",
    "if 'tool_used' in c360_detail.columns:\n",
    "    c360_detail = c360_detail.drop(columns=['tool_used'])\n",
    "\n",
    "# Stage mapping\n",
    "stagefmt_map = {\n",
    "    'Démarche exploratoire/Comprendre le besoin': '11.Démarche exploratoire/Comprendre le besoin',\n",
    "    'Discovery/Understand Needs': '12.Discovery/Understand Needs',\n",
    "    'Review Options': '21.Review Options',\n",
    "    'Present/Gain Commitment': '31.Present/Gain Commitment',\n",
    "    'Intégration commencée': '41.Intégration commencée',\n",
    "    'Onboarding Started': '42.Onboarding Started',\n",
    "    'Opportunity Lost': '51.Opportunity Lost',\n",
    "    'Opportunity Won': '61.Opportunity Won',\n",
    "}\n",
    "if 'stage' in c360_detail.columns:\n",
    "    c360_detail['stage_fmt'] = c360_detail['stage'].map(stagefmt_map).fillna(c360_detail['stage'])\n",
    "\n",
    "# Save\n",
    "c360_detail.to_csv(runday_folder / 'c360_detail.csv', index=False)\n",
    "print(f\"\\nc360_detail rows: {len(c360_detail)}\")\n",
    "\n",
    "# PROC FREQ equivalent for `lob`\n",
    "if 'lob' in c360_detail.columns:\n",
    "    lob_freq = c360_detail['lob'].value_counts(dropna=False).reset_index()\n",
    "    lob_freq.columns = ['lob', 'count']\n",
    "    print(\"\\n[Screenshot 6] lob frequency:\\n\", lob_freq.to_string(index=False))\n",
    "    lob_freq.to_csv(runday_folder / 'c360_detail_lob_freq.csv', index=False)\n",
    "else:\n",
    "    logging.info(\"Column 'lob' not found in c360_detail; skipping frequency table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9856758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Screenshot 7 — AOT pull (14‑day lookback) + link flag on c360\n",
    "# What it does: Pulls AOT events grouped by OPPOR_ID for the window\n",
    "# [wk_start_min14 .. wk_end], makes a distinct OPPOR_ID list, then LEFT JOINs\n",
    "# into `c360_detail` and creates `C360_PDA_Link_AOT` = 1 when\n",
    "#   PROD_CATG_NM == 'Personal Accounts' AND OPPOR_ID is present in AOT.\n",
    "# Also adds a formatted stage name if `oppor_stage_nm` exists via `stagefmt_map`.\n",
    "# Saves outputs as CSV.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a2b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AOT rows: 0 | Unique OPPOR_IDs: 0 | c360_detail_link_aot rows: 0\n"
     ]
    }
   ],
   "source": [
    "wk_start_min14_lit = wk_start_min14.strftime(\"%Y-%m-%d\")\n",
    "wk_end_lit = wk_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "aot_all_oppor = pd.DataFrame(columns=[\"OPPOR_ID\", \"count_aot\"])  # default empty\n",
    "conn = get_teradata_conn()\n",
    "if conn is not None:\n",
    "    try:\n",
    "        sql_aot = f\"\"\"\n",
    "            SELECT OPPOR_ID,\n",
    "                   COUNT(*) AS count_aot\n",
    "            FROM ddw01.evnt_prod_aot\n",
    "            WHERE ess_src_evnt_dt BETWEEN DATE '{wk_start_min14_lit}' AND DATE '{wk_end_lit}'\n",
    "              AND OPPOR_ID IS NOT NULL\n",
    "            GROUP BY 1\n",
    "        \"\"\"\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(sql_aot)\n",
    "            rows = cur.fetchall()\n",
    "            cols = [d[0] for d in cur.description]\n",
    "            aot_all_oppor = pd.DataFrame(rows, columns=cols)\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        logging.error(\"AOT query failed: %s\", e)\n",
    "\n",
    "# Unique OPPOR_ID list\n",
    "aot_all_oppor_unique = aot_all_oppor[[\"OPPOR_ID\"]].drop_duplicates().copy()\n",
    "\n",
    "# Ensure we have c360_detail from prior step\n",
    "if 'c360_detail' not in globals():\n",
    "    c360_detail = pd.DataFrame()\n",
    "\n",
    "# LEFT JOIN on OPPOR_ID\n",
    "c360_detail_link_aot = c360_detail.copy()\n",
    "if not c360_detail_link_aot.empty and 'OPPOR_ID' in c360_detail_link_aot.columns:\n",
    "    c360_detail_link_aot = c360_detail_link_aot.merge(\n",
    "        aot_all_oppor_unique.rename(columns={\"OPPOR_ID\": \"aot_oppor_id\"}),\n",
    "        how=\"left\",\n",
    "        left_on=\"OPPOR_ID\",\n",
    "        right_on=\"aot_oppor_id\",\n",
    "    )\n",
    "else:\n",
    "    logging.info(\"OPPOR_ID not found in c360_detail; skipping AOT join.\")\n",
    "    c360_detail_link_aot['aot_oppor_id'] = None\n",
    "\n",
    "# Add link flag\n",
    "if 'PROD_CATG_NM' not in c360_detail_link_aot.columns:\n",
    "    c360_detail_link_aot['PROD_CATG_NM'] = None\n",
    "\n",
    "c360_detail_link_aot['C360_PDA_Link_AOT'] = (\n",
    "    (c360_detail_link_aot['PROD_CATG_NM'] == 'Personal Accounts') &\n",
    "    c360_detail_link_aot['aot_oppor_id'].notna()\n",
    ").astype(int)\n",
    "\n",
    "# Apply stage mapping if raw column exists\n",
    "if 'oppor_stage_nm' in c360_detail_link_aot.columns:\n",
    "    c360_detail_link_aot['oppor_stage_nm_f'] = (\n",
    "        c360_detail_link_aot['oppor_stage_nm'].map(stagefmt_map)\n",
    "        .fillna(c360_detail_link_aot['oppor_stage_nm'])\n",
    "    )\n",
    "\n",
    "# Save CSV outputs\n",
    "aot_all_oppor.to_csv(runday_folder / 'aot_all_oppor.csv', index=False)\n",
    "aot_all_oppor_unique.to_csv(runday_folder / 'aot_all_oppor_unique.csv', index=False)\n",
    "c360_detail_link_aot.to_csv(runday_folder / 'c360_detail_link_aot.csv', index=False)\n",
    "\n",
    "print(\n",
    "    f\"\\nAOT rows: {len(aot_all_oppor)} | \"\n",
    "    f\"Unique OPPOR_IDs: {len(aot_all_oppor_unique)} | \"\n",
    "    f\"c360_detail_link_aot rows: {len(c360_detail_link_aot)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14a35df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Screenshot 8 — Filter to PRE cohort + validate PA rationale text\n",
    "# What it does:\n",
    "#  1) From `c360_detail_link_aot`, creates two datasets:\n",
    "#     - `c360_detail_more`: a direct copy (for downstream use).\n",
    "#     - `c360_detail_more_in_pre`: subset where all hold:\n",
    "#          asct_prod_fml_nm != 'Risk Protection',\n",
    "#          lob == 'Retail',\n",
    "#          C360_PDA_Link_AOT == 1,\n",
    "#          oppor_stage_nm in ('Opportunity Won','Opportunity Lost').\n",
    "#  2) Builds `pa_rationale` from the PRE subset for rows with\n",
    "#     IS_PROD_APRP_FOR_CLNT == 'Not Appropriate - Rationale'.\n",
    "#     It normalizes rationale text and flags it as VALID when:\n",
    "#        (a) length > 5 chars,\n",
    "#        (b) not just a single repeated character,\n",
    "#        (c) has at least 2 alphanumeric characters.\n",
    "# Saves all outputs as CSVs.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "732e5205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE cohort rows: 0 / 0 total\n",
      "PA rationale rows: 0 (valid: 0)\n"
     ]
    }
   ],
   "source": [
    "# Guard for upstream frame\n",
    "if 'c360_detail_link_aot' not in globals():\n",
    "    c360_detail_link_aot = pd.DataFrame()\n",
    "\n",
    "c360_detail_more = c360_detail_link_aot.copy()\n",
    "\n",
    "# PRE cohort mask (safe even if some columns are missing)\n",
    "mask = pd.Series(True, index=c360_detail_more.index)\n",
    "if 'asct_prod_fml_nm' in c360_detail_more.columns:\n",
    "    mask &= c360_detail_more['asct_prod_fml_nm'].ne('Risk Protection')\n",
    "if 'lob' in c360_detail_more.columns:\n",
    "    mask &= c360_detail_more['lob'].eq('Retail')\n",
    "if 'C360_PDA_Link_AOT' in c360_detail_more.columns:\n",
    "    mask &= c360_detail_more['C360_PDA_Link_AOT'].fillna(0).astype(int).eq(1)\n",
    "if 'oppor_stage_nm' in c360_detail_more.columns:\n",
    "    mask &= c360_detail_more['oppor_stage_nm'].isin(['Opportunity Won', 'Opportunity Lost'])\n",
    "\n",
    "c360_detail_more_in_pre = c360_detail_more[mask].copy()\n",
    "\n",
    "# Save both cohorts\n",
    "c360_detail_more.to_csv(runday_folder / 'c360_detail_more.csv', index=False)\n",
    "c360_detail_more_in_pre.to_csv(runday_folder / 'c360_detail_more_in_pre.csv', index=False)\n",
    "\n",
    "print(f\"PRE cohort rows: {len(c360_detail_more_in_pre)} / {len(c360_detail_more)} total\")\n",
    "\n",
    "# ---- PA rationale extraction & validation ----\n",
    "\n",
    "def normalize_rationale(txt):\n",
    "    if pd.isna(txt):\n",
    "        return ''\n",
    "    # collapse whitespace, trim, uppercase (similar to SAS compress/translate/strip + upcase)\n",
    "    return ' '.join(str(txt).split()).upper()\n",
    "\n",
    "# Select rows that contain PA rationale\n",
    "if not c360_detail_more_in_pre.empty and 'IS_PROD_APRP_FOR_CLNT' in c360_detail_more_in_pre.columns:\n",
    "    pa_mask = c360_detail_more_in_pre['IS_PROD_APRP_FOR_CLNT'].eq('Not Appropriate - Rationale')\n",
    "    pa_rationale = c360_detail_more_in_pre.loc[pa_mask, ['evnt_id', 'IS_PROD_APRP_FOR_CLNT', 'CLNT_RTNL_TXT']].copy()\n",
    "else:\n",
    "    pa_rationale = pd.DataFrame(columns=['evnt_id','IS_PROD_APRP_FOR_CLNT','CLNT_RTNL_TXT'])\n",
    "\n",
    "if not pa_rationale.empty:\n",
    "    pa_rationale['rationale_clean'] = pa_rationale['CLNT_RTNL_TXT'].apply(normalize_rationale)\n",
    "\n",
    "    def is_valid_rationale(s):\n",
    "        s = s or ''\n",
    "        # (a) length > 5\n",
    "        if len(s) <= 5:\n",
    "            return False\n",
    "        # remove spaces for pattern checks\n",
    "        s_no_sp = s.replace(' ', '')\n",
    "        # (b) not only one character repeated\n",
    "        if s_no_sp and len(set(s_no_sp)) == 1:\n",
    "            return False\n",
    "        # (c) at least 2 alphanumeric characters\n",
    "        if sum(ch.isalnum() for ch in s_no_sp) < 2:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    pa_rationale['is_valid_rationale'] = pa_rationale['rationale_clean'].apply(is_valid_rationale)\n",
    "    pa_rationale['prod_not_appr_rtnl_txt_cat'] = pa_rationale['is_valid_rationale'].map({True: 'VALID', False: 'INVALID'})\n",
    "\n",
    "# Save\n",
    "pa_rationale.to_csv(runday_folder / 'pa_rationale.csv', index=False)\n",
    "print(f\"PA rationale rows: {len(pa_rationale)} (valid: {int(pa_rationale.get('is_valid_rationale', pd.Series(dtype=bool)).sum()) if 'is_valid_rationale' in pa_rationale.columns else 0})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b72c4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Screenshot 9 — Merge rationale back + final text category\n",
    "# What it does:\n",
    "#  - LEFT JOINs `pa_rationale` (VALID/INVALID) onto the PRE cohort rows\n",
    "#    (`c360_detail_more_in_pre`) by `evnt_id`.\n",
    "#  - Builds a single label `prod_not_appr_rtnl_txt_cat` with simple rules:\n",
    "#       * missing IS_PROD_APRP_FOR_CLNT  → 'Not Available'\n",
    "#       * IS_PROD_APRP_FOR_CLNT != 'Not Appropriate - Rationale' → 'Not Applicable'\n",
    "#       * otherwise (it equals 'Not Appropriate - Rationale') → use VALID/INVALID\n",
    "#         from `pa_rationale` (defaults to 'INVALID' if missing).\n",
    "#  - Saves `c360_detail_more_in.csv`.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0a64a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c360_detail_more_in rows: 0\n",
      "\n",
      "Rationale category frequency:\n",
      " Empty DataFrame\n",
      "Columns: [prod_not_appr_rtnl_txt_cat, count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Guards\n",
    "if 'c360_detail_more_in_pre' not in globals():\n",
    "    c360_detail_more_in_pre = pd.DataFrame()\n",
    "if 'pa_rationale' not in globals():\n",
    "    pa_rationale = pd.DataFrame(columns=['evnt_id','prod_not_appr_rtnl_txt_cat'])\n",
    "\n",
    "# Prepare merge input\n",
    "if not pa_rationale.empty and 'evnt_id' in pa_rationale.columns:\n",
    "    _to_merge = pa_rationale[['evnt_id','prod_not_appr_rtnl_txt_cat']].copy()\n",
    "else:\n",
    "    _to_merge = pd.DataFrame(columns=['evnt_id','prod_not_appr_rtnl_txt_cat'])\n",
    "\n",
    "# Merge if evnt_id exists in left side, otherwise just copy\n",
    "if 'evnt_id' in c360_detail_more_in_pre.columns:\n",
    "    c360_detail_more_in = c360_detail_more_in_pre.merge(_to_merge, how='left', on='evnt_id')\n",
    "else:\n",
    "    logging.info(\"evnt_id not found in c360_detail_more_in_pre; skipping merge.\")\n",
    "    c360_detail_more_in = c360_detail_more_in_pre.copy()\n",
    "    c360_detail_more_in['prod_not_appr_rtnl_txt_cat'] = None  # placeholder\n",
    "\n",
    "# Final label assignment\n",
    "def final_rationale_label(row):\n",
    "    v = row['IS_PROD_APRP_FOR_CLNT'] if 'IS_PROD_APRP_FOR_CLNT' in row else None\n",
    "    if pd.isna(v) or str(v).strip() == '':\n",
    "        return 'Not Available'\n",
    "    if str(v).strip().upper() != 'NOT APPROPRIATE - RATIONALE':\n",
    "        return 'Not Applicable'\n",
    "    # if it is 'Not Appropriate - Rationale', use the VALID/INVALID value\n",
    "    cat = row.get('prod_not_appr_rtnl_txt_cat')\n",
    "    return cat if isinstance(cat, str) and cat else 'INVALID'\n",
    "\n",
    "c360_detail_more_in['prod_not_appr_rtnl_txt_cat'] = c360_detail_more_in.apply(final_rationale_label, axis=1)\n",
    "\n",
    "# Save\n",
    "c360_detail_more_in.to_csv(runday_folder / 'c360_detail_more_in.csv', index=False)\n",
    "print(f\"c360_detail_more_in rows: {len(c360_detail_more_in)}\")\n",
    "\n",
    "# PROC FREQ equivalent\n",
    "if 'prod_not_appr_rtnl_txt_cat' in c360_detail_more_in.columns:\n",
    "    freq = c360_detail_more_in['prod_not_appr_rtnl_txt_cat'].value_counts(dropna=False).reset_index()\n",
    "    freq.columns = ['prod_not_appr_rtnl_txt_cat','count']\n",
    "    print(\"\\nRationale category frequency:\\n\", freq.to_string(index=False))\n",
    "    freq.to_csv(runday_folder / 'c360_detail_more_in_rationale_freq.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b4d3c",
   "metadata": {},
   "source": [
    "Screenshot 10 — “Comment map + per-opportunity counter”\n",
    "\n",
    "What it does (in short):\n",
    "\n",
    "Adds a Python dict that mirrors the SAS $cs_cmt format mapping (COM1…COM19 → readable comments like “Test population (less samples)”).\n",
    "\n",
    "Sorts c360_detail_more_in by OPPOR_ID and creates level_oppor = row number within each opportunity (1,2,3,…) — equivalent to the SAS first.OPPOR_ID + level_oppor+1 pattern.\n",
    "\n",
    "Saves c360_detail_more_in_level.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fec8ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Leveled rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Comment/CS mapping (SAS $cs_cmt → Python dict)\n",
    "cs_cmt_map = {\n",
    "    'COM1':  'Test population (less samples)',\n",
    "    'COM2':  'Match population',\n",
    "    'COM3':  'Mismatch population (less samples)',\n",
    "    'COM4':  'Non Anomaly Population',\n",
    "    'COM5':  'Anomaly Population',\n",
    "    'COM6':  'Number of Deposit Sessions',\n",
    "    'COM7':  'Number of Accounts',\n",
    "    'COM8':  'Number of Transactions',\n",
    "    'COM9':  'Non Blank Population',\n",
    "    'COM10': 'Blank Population',\n",
    "    'COM11': 'Unable to Assess',\n",
    "    'COM12': 'Number of Failed Data Elements',\n",
    "    'COM13': 'Population Distribution',\n",
    "    'COM14': 'Reconciled Population',\n",
    "    'COM15': 'Not Reconciled Population',\n",
    "    'COM16': 'Pass',\n",
    "    'COM17': 'Fail',\n",
    "    'COM18': 'Not Applicable',\n",
    "    'COM19': 'Potential Fail',\n",
    "}\n",
    "\n",
    "# Guard\n",
    "if 'c360_detail_more_in' not in globals():\n",
    "    c360_detail_more_in = pd.DataFrame()\n",
    "\n",
    "c360_detail_more_in_leveled = c360_detail_more_in.copy()\n",
    "\n",
    "if not c360_detail_more_in_leveled.empty:\n",
    "    # Apply mapping if column exists\n",
    "    if 'comment_code' in c360_detail_more_in_leveled.columns:\n",
    "        c360_detail_more_in_leveled['comment_text'] = (\n",
    "            c360_detail_more_in_leveled['comment_code'].map(cs_cmt_map)\n",
    "        )\n",
    "\n",
    "    # Add row-level counter per OPPOR_ID\n",
    "    if 'OPPOR_ID' in c360_detail_more_in_leveled.columns:\n",
    "        c360_detail_more_in_leveled = (\n",
    "            c360_detail_more_in_leveled.sort_values(['OPPOR_ID'])\n",
    "            .assign(level_oppor=lambda d: d.groupby('OPPOR_ID').cumcount() + 1)\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\"Column 'OPPOR_ID' not found, skipping level_oppor generation.\")\n",
    "\n",
    "# Save\n",
    "c360_detail_more_in_leveled.to_csv(runday_folder / 'c360_detail_more_in_level.csv', index=False)\n",
    "print(f\"\\nLeveled rows: {len(c360_detail_more_in_leveled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c416b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Screenshot 11 — Final shaped view for audit/export\n",
    "# What it does:\n",
    "#  - Takes the first row per opportunity (where level_oppor == 1).\n",
    "#  - Adds friendly report fields and segments (constants + sourced columns).\n",
    "#  - Derives date fields: `segment10` as YYYYMM from evnt_dt, `SnapDate` as end\n",
    "#    of week of evnt_dt, and `DateCompleted` as today.\n",
    "#  - Builds comment fields from `CommentCode` using `cs_cmt_map`.\n",
    "#  - Saves the shaped table and a PROC FREQ–style summary of `PROD_CATG_NM`.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a043e05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_pa_c360_4ac rows: 0\n",
      "PROD_CATG_NM frequency saved.\n"
     ]
    }
   ],
   "source": [
    "# Input: leveled cohort from Above (sS-10)\n",
    "src = 'c360_detail_more_in_leveled' if 'c360_detail_more_in_leveled' in globals() else (\n",
    "      'c360_detail_more_in' if 'c360_detail_more_in' in globals() else None)\n",
    "\n",
    "if src is None:\n",
    "    tmp_pa_c360_4ac = pd.DataFrame()\n",
    "else:\n",
    "    df0 = globals()[src].copy()\n",
    "\n",
    "    # filter only first rows per OPPOR_ID if column exists\n",
    "    if 'level_oppor' in df0.columns:\n",
    "        df = df0[df0['level_oppor'].eq(1)].copy()\n",
    "    else:\n",
    "        df = df0.copy()\n",
    "\n",
    "    # Safe datetime parse for evnt_dt\n",
    "    if 'evnt_dt' in df.columns:\n",
    "        evnt_dt_parsed = pd.to_datetime(df['evnt_dt'], errors='coerce')\n",
    "    else:\n",
    "        evnt_dt_parsed = pd.to_datetime(pd.Series([], dtype='datetime64[ns]'))\n",
    "\n",
    "    # Constants\n",
    "    df['RegulatoryName'] = 'C86'\n",
    "    df['LOB'] = 'Retail'  # constant per the SAS snippet\n",
    "    df['ReportName'] = 'C86 Client360 Product Appropriateness'\n",
    "    df['ControlRisk'] = 'Completeness'\n",
    "    df['TestType'] = 'Anomaly'\n",
    "    df['TestPeriod'] = 'Origination'\n",
    "\n",
    "    # ProductType → Product Category (try multiple candidates)\n",
    "    prod_cat_col = None\n",
    "    for c in ['ASCT_PROD_FMLY_NM', 'asct_prod_fml_nm', 'asct_prod_fmly_nm', 'PROD_CATG_NM']:\n",
    "        if c in df.columns:\n",
    "            prod_cat_col = c\n",
    "            break\n",
    "    df['ProductType'] = df[prod_cat_col] if prod_cat_col else ''\n",
    "\n",
    "    # Segments (safe guards for missing cols)\n",
    "    df['segment1'] = 'Account Open'\n",
    "    df['segment2'] = df['ProductType']\n",
    "    df['segment3'] = df['PROD_SRVC_NM'] if 'PROD_SRVC_NM' in df.columns else ''\n",
    "    df['segment6'] = df['oppor_stage_nm'] if 'oppor_stage_nm' in df.columns else ''\n",
    "    df['segment7'] = df['TOOL_USED'] if 'TOOL_USED' in df.columns else ''\n",
    "\n",
    "    # segment10 = YYYYMM from event date\n",
    "    df['segment10'] = evnt_dt_parsed.dt.strftime('%Y%m') if len(evnt_dt_parsed) else ''\n",
    "\n",
    "    # Comments\n",
    "    df['CommentCode'] = 'COM13'\n",
    "    df['Comments'] = df['CommentCode'].map(cs_cmt_map)\n",
    "\n",
    "    # Holdout flag\n",
    "    df['HoldoutFlag'] = 'N'\n",
    "\n",
    "    # Dates\n",
    "    if len(evnt_dt_parsed):\n",
    "        dow = evnt_dt_parsed.dt.weekday  # Monday=0..Sunday=6\n",
    "        df['SnapDate'] = (evnt_dt_parsed + pd.to_timedelta(6 - dow, unit='D')).dt.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        df['SnapDate'] = ''\n",
    "    df['DateCompleted'] = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Final shaped view\n",
    "    keep_cols = [\n",
    "        'OPPOR_ID', 'evnt_id', 'evnt_dt', 'LOB', 'RegulatoryName', 'ReportName',\n",
    "        'ControlRisk', 'TestType', 'TestPeriod', 'ProductType',\n",
    "        'segment1','segment2','segment3','segment6','segment7','segment10',\n",
    "        'CommentCode','Comments','HoldoutFlag','SnapDate','DateCompleted'\n",
    "    ]\n",
    "    keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "    tmp_pa_c360_4ac = df[keep_cols].copy()\n",
    "\n",
    "# Save shaped table\n",
    "tmp_pa_c360_4ac.to_csv(runday_folder / 'tmp_pa_c360_4ac.csv', index=False)\n",
    "print(f\"tmp_pa_c360_4ac rows: {len(tmp_pa_c360_4ac)}\")\n",
    "\n",
    "# PROC FREQ–style summary for PROD_CATG_NM\n",
    "if src is not None and 'PROD_CATG_NM' in globals()[src].columns:\n",
    "    freq_prod = globals()[src]['PROD_CATG_NM'].value_counts(dropna=False).reset_index()\n",
    "    freq_prod.columns = ['PROD_CATG_NM','count']\n",
    "    freq_prod.to_csv(runday_folder / 'prod_catg_freq.csv', index=False)\n",
    "    print(\"PROD_CATG_NM frequency saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06377f93",
   "metadata": {},
   "source": [
    "Screenshot 12 — “AC assessment roll-up (segment labels + counts)”\n",
    "\n",
    "What it does (in short):\n",
    "\n",
    "Starts from tmp_pa_c360_4ac and left joins IS_PROD_APRP_FOR_CLNT and prod_not_appr_rtnl_txt_cat (from c360_detail_more_in) by evnt_id.\n",
    "\n",
    "Adds RDE='PA002_Client360_Completeness_RDE'.\n",
    "\n",
    "Builds segment4 from IS_PROD_APRP_FOR_CLNT:\n",
    "\n",
    "outside C360 → Product Appropriateness assessed outside Client 360\n",
    "\n",
    "Not Appropriate – Rationale → Product Not Appropriate\n",
    "\n",
    "Client declined… → Client declined product appropriateness assessment\n",
    "\n",
    "Product Appropriate → Product Appropriate\n",
    "\n",
    "else → Missing\n",
    "\n",
    "Sets segment5 = prod_not_appr_rtnl_txt_cat.\n",
    "\n",
    "Groups by the report/segment fields and outputs counts (volume and a simple Amount = volume).\n",
    "\n",
    "Saves: tmp_pa_c360_ac_assessment.csv and a PROC FREQ–style segment4_freq.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3cdb4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assessment rows: 0\n",
      "segment4 frequency saved.\n"
     ]
    }
   ],
   "source": [
    "# Ensure required DataFrames exist\n",
    "if 'tmp_pa_c360_4ac' not in globals():\n",
    "    tmp_pa_c360_4ac = pd.DataFrame()\n",
    "if 'c360_detail_more_in' not in globals():\n",
    "    c360_detail_more_in = pd.DataFrame()\n",
    "\n",
    "# Start from a copy of the base DataFrame\n",
    "base = tmp_pa_c360_4ac.copy()\n",
    "\n",
    "# Bring in needed columns from the detailed cohort\n",
    "merge_cols = ['evnt_id', 'IS_PROD_APRP_FOR_CLNT', 'prod_not_appr_rtnl_txt_cat']\n",
    "right = c360_detail_more_in[[c for c in merge_cols if c in c360_detail_more_in.columns]].copy()\n",
    "\n",
    "if not right.empty:\n",
    "    base = base.merge(right, how='left', on='evnt_id')\n",
    "else:\n",
    "    # Ensure columns exist even if right is empty\n",
    "    for c in merge_cols[1:]:\n",
    "        if c not in base.columns:\n",
    "            base[c] = None\n",
    "\n",
    "# Constants and segment derivations\n",
    "base['RDE'] = 'PA002_Client360_Completeness_RDE'\n",
    "\n",
    "# Map segment4 values\n",
    "map_seg4 = {\n",
    "    'Product Appropriateness assessed outside Client 360': 'Product Appropriateness assessed outside Client 360',\n",
    "    'Not Appropriate - Rationale': 'Product Not Appropriate',\n",
    "    'Client declined product appropriateness assessment': 'Client declined product appropriateness assessment',\n",
    "    'Product Appropriate': 'Product Appropriate',\n",
    "}\n",
    "base['segment4'] = base['IS_PROD_APRP_FOR_CLNT'].map(map_seg4).fillna('Missing')\n",
    "\n",
    "# segment5 is the rationale text category\n",
    "base['segment5'] = base.get('prod_not_appr_rtnl_txt_cat', None)\n",
    "\n",
    "# Columns to group by (only include those present)\n",
    "group_cols_all = [\n",
    "    'RegulatoryName','LOB','ReportName','ControlRisk','TestType','TestPeriod','ProductType',\n",
    "    'RDE','segment1','segment2','segment3','segment4','segment5','segment6','segment7',\n",
    "    'segment10','HoldoutFlag','CommentCode','Comments','DateCompleted','SnapDate'\n",
    "]\n",
    "group_cols = [c for c in group_cols_all if c in base.columns]\n",
    "\n",
    "# Aggregate/roll-up\n",
    "if group_cols:\n",
    "    rollup = base.groupby(group_cols, dropna=False).size().reset_index(name='volume')\n",
    "    rollup['Amount'] = rollup['volume']  # beginner-friendly roll-up\n",
    "    tmp_pa_c360_ac_assessment = rollup\n",
    "else:\n",
    "    tmp_pa_c360_ac_assessment = pd.DataFrame()\n",
    "\n",
    "# Save outputs\n",
    "tmp_pa_c360_ac_assessment.to_csv(runday_folder / 'tmp_pa_c360_ac_assessment.csv', index=False)\n",
    "print(f\"assessment rows: {len(tmp_pa_c360_ac_assessment)}\")\n",
    "\n",
    "# PROC FREQ-style count of segment4\n",
    "if 'segment4' in base.columns:\n",
    "    seg4_freq = base['segment4'].value_counts(dropna=False).reset_index()\n",
    "    seg4_freq.columns = ['segment4','count']\n",
    "    seg4_freq.to_csv(runday_folder / 'segment4_freq.csv', index=False)\n",
    "    print(\"segment4 frequency saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f3a395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Screenshot 13 — Output template + aligned export\n",
    "# What it does:\n",
    "#  - Defines a fixed column order/types (like SAS template dataset).\n",
    "#  - Takes `tmp_pa_c360_ac_assessment` and reindexes/renames to match the\n",
    "#    template, filling missing columns with blanks/zeros.\n",
    "#  - Formats dates (`DateCompleted`, `SnapDate`) as YYYY-MM-DD (similar to yymmdd10.).\n",
    "#  - Saves final export `pa_c360_autocomplete_tool_use.csv`.\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8594a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final export rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Guard\n",
    "if 'tmp_pa_c360_ac_assessment' not in globals():\n",
    "    tmp_pa_c360_ac_assessment = pd.DataFrame()\n",
    "\n",
    "# Desired column order (Python version of the SAS template)\n",
    "TEMPLATE_COLS = [\n",
    "    'RegulatoryName','LOB','ReportName','ControlRisk','TestType','TestPeriod','ProductType',\n",
    "    'RDE','segment','segment2','segment3','segment4','segment5','segment6','segment7','segment8','segment9','segment10',\n",
    "    'HoldoutFlag','CommentCode','Comments','DateCompleted','SnapDate','volume','Amount'\n",
    "]\n",
    "\n",
    "# Start from the assessment roll-up\n",
    "export_df = tmp_pa_c360_ac_assessment.copy()\n",
    "\n",
    "# In our earlier steps we used 'volume' (lowercase) already; make sure it's there\n",
    "if 'volume' not in export_df.columns and 'Volume' in export_df.columns:\n",
    "    export_df.rename(columns={'Volume':'volume'}, inplace=True)\n",
    "\n",
    "# Map segment1 → segment if needed\n",
    "if 'segment' not in export_df.columns and 'segment1' in export_df.columns:\n",
    "    export_df['segment'] = export_df['segment1']\n",
    "\n",
    "# Ensure all template columns exist\n",
    "for c in TEMPLATE_COLS:\n",
    "    if c not in export_df.columns:\n",
    "        export_df[c] = '' if c not in ('volume','Amount') else 0\n",
    "\n",
    "# Coerce simple types\n",
    "export_df['volume'] = pd.to_numeric(export_df['volume'], errors='coerce').fillna(0).astype(int)\n",
    "export_df['Amount'] = pd.to_numeric(export_df['Amount'], errors='coerce').fillna(export_df['volume']).astype(int)\n",
    "\n",
    "# Date formatting (YYYY-MM-DD ~ yymmdd10.)\n",
    "for dc in ['DateCompleted','SnapDate']:\n",
    "    if dc in export_df.columns:\n",
    "        export_df[dc] = pd.to_datetime(export_df[dc], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Final order\n",
    "export_df = export_df[TEMPLATE_COLS]\n",
    "\n",
    "# Save\n",
    "export_df.to_csv(runday_folder / 'pa_c360_autocomplete_tool_use.csv', index=False)\n",
    "print(f\"\\nFinal export rows: {len(export_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90beaeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tmp_pa_c360_4ac_count rows: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Screenshot 14 — Prepare data for 'tool used' count (add segment8 = tool name)\n",
    "# =============================================================================\n",
    "\n",
    "# Ensure required DataFrames exist\n",
    "if 'tmp_pa_c360_4ac' not in globals():\n",
    "    tmp_pa_c360_4ac = pd.DataFrame()\n",
    "if 'tracking_tool_use_distinct' not in globals():\n",
    "    tracking_tool_use_distinct = pd.DataFrame(columns=['OPPOR_ID','ADVC_TOOL_NM'])\n",
    "\n",
    "# Copy base DataFrames\n",
    "left_df = tmp_pa_c360_4ac.copy()\n",
    "right_df = tracking_tool_use_distinct.copy()\n",
    "\n",
    "# Ensure OPPOR_ID exists\n",
    "for df in [left_df, right_df]:\n",
    "    if 'OPPOR_ID' not in df.columns:\n",
    "        df['OPPOR_ID'] = None\n",
    "\n",
    "# Merge and create segment8\n",
    "count_pre = left_df.merge(\n",
    "    right_df[['OPPOR_ID','ADVC_TOOL_NM']].drop_duplicates(),\n",
    "    how='left',\n",
    "    on='OPPOR_ID'\n",
    ")\n",
    "\n",
    "count_pre['segment8'] = count_pre.get('ADVC_TOOL_NM', '').astype(str).str.upper()\n",
    "\n",
    "# Fill in default values for mandatory columns\n",
    "defaults = {\n",
    "    'RegulatoryName': 'C86',\n",
    "    'LOB': 'Retail',\n",
    "    'ReportName': 'C86 Client360 Product Appropriateness',\n",
    "    'ControlRisk': 'Completeness',\n",
    "    'TestType': 'Anomaly',\n",
    "    'TestPeriod': 'Origination'\n",
    "}\n",
    "for col, default in defaults.items():\n",
    "    count_pre[col] = count_pre.get(col, default)\n",
    "\n",
    "# SnapDate calculation (end of week)\n",
    "if 'SnapDate' not in count_pre.columns and 'evnt_dt' in count_pre.columns:\n",
    "    evnt_dt_parsed = pd.to_datetime(count_pre['evnt_dt'], errors='coerce')\n",
    "    dow = evnt_dt_parsed.dt.weekday\n",
    "    count_pre['SnapDate'] = (evnt_dt_parsed + pd.to_timedelta(6 - dow, unit='D')).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# DateCompleted default to today\n",
    "if 'DateCompleted' not in count_pre.columns:\n",
    "    count_pre['DateCompleted'] = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Save output\n",
    "count_pre.to_csv(runday_folder / 'tmp_pa_c360_4ac_count.csv', index=False)\n",
    "print(f\"\\n tmp_pa_c360_4ac_count rows: {len(count_pre)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2990bf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac_count_assessment rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Ensure c360_detail_more_in exists\n",
    "if 'c360_detail_more_in' not in globals():\n",
    "    c360_detail_more_in = pd.DataFrame()\n",
    "\n",
    "# Start from a copy of the tool-used dataset\n",
    "base2 = count_pre.copy()\n",
    "\n",
    "# Columns to bring in from the detailed cohort\n",
    "right2_cols = ['evnt_id','IS_PROD_APRP_FOR_CLNT','prod_not_appr_rtnl_txt_cat']\n",
    "right2 = c360_detail_more_in[[c for c in right2_cols if c in c360_detail_more_in.columns]].copy()\n",
    "\n",
    "# Merge or create missing columns\n",
    "if not right2.empty:\n",
    "    base2 = base2.merge(right2, how='left', on='evnt_id')\n",
    "else:\n",
    "    for c in right2_cols[1:]:\n",
    "        if c not in base2.columns:\n",
    "            base2[c] = None\n",
    "\n",
    "# Constants and segment derivations\n",
    "base2['RDE'] = 'PA003_Client360_Completeness_Tool'\n",
    "\n",
    "# Map segment4 values using existing map_seg4\n",
    "base2['segment4'] = base2.get('IS_PROD_APRP_FOR_CLNT', None).map(map_seg4).fillna('Missing')\n",
    "\n",
    "# segment5 is the rationale text category\n",
    "base2['segment5'] = base2.get('prod_not_appr_rtnl_txt_cat', None)\n",
    "\n",
    "# Columns to group by (only include those present)\n",
    "count_group_cols_all = [\n",
    "    'RegulatoryName','LOB','ReportName','ControlRisk','TestType','TestPeriod','ProductType',\n",
    "    'RDE','segment1','segment2','segment3','segment4','segment5','segment6','segment7',\n",
    "    'segment8','segment10','HoldoutFlag','CommentCode','Comments','DateCompleted','SnapDate'\n",
    "]\n",
    "count_group_cols = [c for c in count_group_cols_all if c in base2.columns]\n",
    "\n",
    "# Aggregate / roll-up\n",
    "if count_group_cols:\n",
    "    count_rollup = base2.groupby(count_group_cols, dropna=False).size().reset_index(name='volume')\n",
    "    count_rollup['Amount'] = count_rollup['volume']  # beginner-friendly roll-up\n",
    "    tmp_pa_c360_ac_count_assessment = count_rollup\n",
    "else:\n",
    "    tmp_pa_c360_ac_count_assessment = pd.DataFrame()\n",
    "\n",
    "# Save output\n",
    "tmp_pa_c360_ac_count_assessment.to_csv(runday_folder / 'tmp_pa_c360_ac_count_assessment.csv', index=False)\n",
    "print(f\"ac_count_assessment rows: {len(tmp_pa_c360_ac_count_assessment)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "417f3090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined rows: 0 | final table rows: 0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Screenshot 16 — Align tool-count assessment to template and combine\n",
    "# -------------------------------\n",
    "\n",
    "# Ensure tmp_pa_c360_ac_count_assessment exists\n",
    "if 'tmp_pa_c360_ac_count_assessment' not in globals():\n",
    "    tmp_pa_c360_ac_count_assessment = pd.DataFrame()\n",
    "\n",
    "# Segment4 frequency for tool-count assessment\n",
    "if 'segment4' in tmp_pa_c360_ac_count_assessment.columns:\n",
    "    seg4_count_freq = tmp_pa_c360_ac_count_assessment['segment4'].value_counts(dropna=False).reset_index()\n",
    "    seg4_count_freq.columns = ['segment4', 'count']\n",
    "    seg4_count_freq.to_csv(runday_folder / 'segment4_freq_count.csv', index=False)\n",
    "\n",
    "# Template columns\n",
    "if 'TEMPLATE_COLS' not in globals():\n",
    "    TEMPLATE_COLS = [\n",
    "        'RegulatoryName','LOB','ReportName','ControlRisk','TestType','TestPeriod','ProductType',\n",
    "        'RDE','segment','segment2','segment3','segment4','segment5','segment6','segment7','segment8','segment9','segment10',\n",
    "        'HoldoutFlag','CommentCode','Comments','DateCompleted','SnapDate','volume','Amount'\n",
    "    ]\n",
    "\n",
    "# Align tmp_pa_c360_ac_count_assessment to template\n",
    "pa_c360_autocomplete_Count_Tool = tmp_pa_c360_ac_count_assessment.copy()\n",
    "if 'segment' not in pa_c360_autocomplete_Count_Tool.columns and 'segment1' in pa_c360_autocomplete_Count_Tool.columns:\n",
    "    pa_c360_autocomplete_Count_Tool['segment'] = pa_c360_autocomplete_Count_Tool['segment1']\n",
    "\n",
    "# Ensure all template columns exist\n",
    "for c in TEMPLATE_COLS:\n",
    "    if c not in pa_c360_autocomplete_Count_Tool.columns:\n",
    "        pa_c360_autocomplete_Count_Tool[c] = '' if c not in ('volume','Amount') else 0\n",
    "\n",
    "# Ensure numeric columns are correct\n",
    "pa_c360_autocomplete_Count_Tool['volume'] = pd.to_numeric(pa_c360_autocomplete_Count_Tool['volume'], errors='coerce').fillna(0).astype(int)\n",
    "pa_c360_autocomplete_Count_Tool['Amount'] = pd.to_numeric(pa_c360_autocomplete_Count_Tool['Amount'], errors='coerce')\\\n",
    "                                            .fillna(pa_c360_autocomplete_Count_Tool['volume']).astype(int)\n",
    "\n",
    "# Standardize date columns\n",
    "for dc in ['DateCompleted','SnapDate']:\n",
    "    if dc in pa_c360_autocomplete_Count_Tool.columns:\n",
    "        pa_c360_autocomplete_Count_Tool[dc] = pd.to_datetime(pa_c360_autocomplete_Count_Tool[dc], errors='coerce')\\\n",
    "                                              .dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Reorder to template\n",
    "pa_c360_autocomplete_Count_Tool = pa_c360_autocomplete_Count_Tool[TEMPLATE_COLS]\n",
    "\n",
    "# Load tool-use export\n",
    "if 'export_df' in globals():\n",
    "    pa_c360_autocomplete_tool_use = export_df.copy()\n",
    "else:\n",
    "    tool_use_path = runday_folder / 'pa_c360_autocomplete_tool_use.csv'\n",
    "    if tool_use_path.exists():\n",
    "        pa_c360_autocomplete_tool_use = pd.read_csv(tool_use_path)\n",
    "    else:\n",
    "        pa_c360_autocomplete_tool_use = pd.DataFrame(columns=TEMPLATE_COLS)\n",
    "\n",
    "# Ensure all template columns exist for export\n",
    "for c in TEMPLATE_COLS:\n",
    "    if c not in pa_c360_autocomplete_tool_use.columns:\n",
    "        pa_c360_autocomplete_tool_use[c] = '' if c not in ('volume','Amount') else 0\n",
    "pa_c360_autocomplete_tool_use = pa_c360_autocomplete_tool_use[TEMPLATE_COLS]\n",
    "\n",
    "# Combine count-tool assessment with tool-use export\n",
    "combine_pa_autocomplete = pd.concat([pa_c360_autocomplete_Count_Tool, pa_c360_autocomplete_tool_use], ignore_index=True)\n",
    "combine_pa_autocomplete.to_csv(runday_folder / 'combine_pa_autocomplete.csv', index=False)\n",
    "\n",
    "# Append into base table (replace rows for today's DateCompleted)\n",
    "base_path = outpath / 'pa_client360_autocomplete.csv'\n",
    "if base_path.exists():\n",
    "    base_df = pd.read_csv(base_path)\n",
    "else:\n",
    "    base_df = pd.DataFrame(columns=TEMPLATE_COLS)\n",
    "\n",
    "# Ensure all template columns exist in base\n",
    "for c in TEMPLATE_COLS:\n",
    "    if c not in base_df.columns:\n",
    "        base_df[c] = '' if c not in ('volume','Amount') else 0\n",
    "\n",
    "# Filter out today's rows and append\n",
    "runday_date_str = pd.to_datetime(runday, format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "if 'DateCompleted' in base_df.columns:\n",
    "    base_df = base_df[base_df['DateCompleted'] != runday_date_str]\n",
    "\n",
    "final_autocomplete = pd.concat([base_df[TEMPLATE_COLS], combine_pa_autocomplete[TEMPLATE_COLS]], ignore_index=True)\n",
    "final_autocomplete.to_csv(base_path, index=False)\n",
    "\n",
    "print(f\"combined rows: {len(combine_pa_autocomplete)} | final table rows: {len(final_autocomplete)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3cdc395c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detail rows: 0 | Excel: pa_client360_autocomplete.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Screenshot 17 — Export autocomplete workbook and build daily detail extract\n",
    "# -------------------------------\n",
    "\n",
    "excel_path = outpath / 'pa_client360_autocomplete.xlsx'\n",
    "base_csv_path = outpath / 'pa_client360_autocomplete.csv'\n",
    "\n",
    "# Ensure final_autocomplete exists\n",
    "if 'final_autocomplete' not in globals():\n",
    "    if base_csv_path.exists():\n",
    "        final_autocomplete = pd.read_csv(base_csv_path)\n",
    "    else:\n",
    "        final_autocomplete = pd.DataFrame()\n",
    "\n",
    "# Prepare detail build from enriched cohort\n",
    "detail_src = c360_detail_more_in.copy() if 'c360_detail_more_in' in globals() else pd.DataFrame()\n",
    "\n",
    "# Merge in tool use if available\n",
    "if 'tracking_tool_use_distinct' in globals() and not detail_src.empty:\n",
    "    _tool = tracking_tool_use_distinct[['OPPOR_ID','ADVC_TOOL_NM']].drop_duplicates()\n",
    "    detail_src = detail_src.merge(_tool, how='left', on='OPPOR_ID')\n",
    "else:\n",
    "    if 'ADVC_TOOL_NM' not in detail_src.columns:\n",
    "        detail_src['ADVC_TOOL_NM'] = ''\n",
    "\n",
    "# Ensure all columns exist to avoid KeyErrors\n",
    "for col in ['evnt_dt','EVNT_TMSTP','DateCompleted','OPPOR_ID','OPPOR_REC_TYP','PROD_CD',\n",
    "            'PROD_CATG_NM','ASCT_PROD_FMLY_NM','PROD_SRVC_NM','oppor_stage_nm','TOOL_USED',\n",
    "            'tool_used','IS_PROD_APRP_FOR_CLNT','CLNT_RTNL_TXT','prod_not_appr_rtnl_txt_cat',\n",
    "            'RBC_OPPOR_OWN_ID','OCCPT_JOB_CD','HR_POSN_TITL_EN','ORG_UNT_NO','POSN_STRT_DT']:\n",
    "    if col not in detail_src.columns:\n",
    "        detail_src[col] = pd.NA\n",
    "\n",
    "# Safe datetime conversions\n",
    "_ev = pd.to_datetime(detail_src['evnt_dt'], errors='coerce')\n",
    "_event_month = _ev.dt.strftime('%Y%m')\n",
    "_week_end = (_ev + pd.to_timedelta(6 - _ev.dt.weekday, unit='D')).dt.strftime('%Y-%m-%d')\n",
    "_event_date = _ev.dt.strftime('%Y-%m-%d')\n",
    "\n",
    "_ts = pd.to_datetime(detail_src['EVNT_TMSTP'], errors='coerce').fillna(_ev)\n",
    "_ts = _ts.dt.strftime('%Y-%m-%d %I:%M:%S %p')\n",
    "\n",
    "# Map PA results\n",
    "_pa_map = {\n",
    "    'Product Appropriateness assessed outside Client 360': 'Product Appropriateness assessed outside Client 360',\n",
    "    'Not Appropriate - Rationale': 'Product Not Appropriate',\n",
    "    'Client declined product appropriateness assessment': 'Client declined product appropriateness assessment',\n",
    "    'Product Appropriate': 'Product Appropriate',\n",
    "}\n",
    "\n",
    "# Build detail dataframe\n",
    "detail_df = pd.DataFrame({\n",
    "    'event_month': _event_month,\n",
    "    'reporting_date': pd.to_datetime(detail_src['DateCompleted'], errors='coerce').dt.strftime('%Y-%m-%d'),\n",
    "    'event_week_ending': _week_end,\n",
    "    'event_date': _event_date,\n",
    "    'event_timestamp': _ts,\n",
    "    'opportunity_id': detail_src['OPPOR_ID'],\n",
    "    'opportunity_type': detail_src['OPPOR_REC_TYP'],\n",
    "    'product_code': detail_src['PROD_CD'],\n",
    "    'product_category_name': detail_src['PROD_CATG_NM'],\n",
    "    'product_family_name': detail_src['ASCT_PROD_FMLY_NM'],\n",
    "    'product_name': detail_src['PROD_SRVC_NM'],\n",
    "    'oppor_stage_nm': detail_src['oppor_stage_nm'],\n",
    "    'tool_used': detail_src['TOOL_USED'].where(detail_src['TOOL_USED'].notna(), detail_src['tool_used']),\n",
    "    'tool_nm': detail_src['ADVC_TOOL_NM'],\n",
    "    'PA_result': detail_src['IS_PROD_APRP_FOR_CLNT'].map(_pa_map).fillna('Missing'),\n",
    "    'PA_rationale': detail_src['CLNT_RTNL_TXT'],\n",
    "    'PA_rationale_validity': detail_src['prod_not_appr_rtnl_txt_cat'],\n",
    "    'employee_id': detail_src['RBC_OPPOR_OWN_ID'],\n",
    "    'job_code': detail_src['OCCPT_JOB_CD'],\n",
    "    'position_title': detail_src['HR_POSN_TITL_EN'],\n",
    "    'employee_transit': detail_src['ORG_UNT_NO'],\n",
    "    'position_start_date': pd.to_datetime(detail_src['POSN_STRT_DT'], errors='coerce').dt.strftime('%Y-%m-%d'),\n",
    "})\n",
    "\n",
    "# Save detail CSV\n",
    "_detail_csv = outpath / f'pa_client360_detail_{runday}.csv'\n",
    "detail_df.to_csv(_detail_csv, index=False)\n",
    "\n",
    "# Save Excel workbook with autocomplete and detail\n",
    "with pd.ExcelWriter(excel_path) as _writer:\n",
    "    final_autocomplete.to_excel(_writer, index=False, sheet_name='autocomplete')\n",
    "    detail_df.to_excel(_writer, index=False, sheet_name=f'detail_{runday}')\n",
    "\n",
    "print(f\"detail rows: {len(detail_df)} | Excel: {excel_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c70c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered detail rows: 0 | detail workbook: pa_client360_detail_20250910.xlsx | pivot workbook: PA_Client36e_Pivot.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Screenshot 18 — Filtered detail export and pivot update\n",
    "# -------------------------------\n",
    "\n",
    "import os, time\n",
    "\n",
    "# Filtered PA_result values\n",
    "filter_vals = [\n",
    "    'Product Not Appropriate',\n",
    "    'Missing',\n",
    "    'Product Appropriateness assessed outside Client 360',\n",
    "]\n",
    "\n",
    "# Ensure detail_df exists\n",
    "if 'detail_df' not in globals():\n",
    "    detail_df = pd.DataFrame()\n",
    "\n",
    "# Filter rows safely\n",
    "if not detail_df.empty:\n",
    "    filtered_detail = detail_df[detail_df.get('PA_result', pd.Series(dtype=str)).isin(filter_vals)].copy()\n",
    "else:\n",
    "    filtered_detail = pd.DataFrame()\n",
    "\n",
    "# Write filtered detail workbook in run-day folder\n",
    "_detail_xlsx = runday_folder / f'pa_client360_detail_{runday}.xlsx'\n",
    "with pd.ExcelWriter(_detail_xlsx) as writer:\n",
    "    filtered_detail.to_excel(writer, index=False, sheet_name='detail')\n",
    "\n",
    "# Update pivot workbook with autocomplete sheet\n",
    "_pivot_xlsx = outpath / 'PA_Client360_Pivot.xlsx'\n",
    "with pd.ExcelWriter(_pivot_xlsx) as writer:\n",
    "    final_autocomplete_safe = final_autocomplete if 'final_autocomplete' in globals() else pd.DataFrame()\n",
    "    final_autocomplete_safe.to_excel(writer, index=False, sheet_name='Autocomplete')\n",
    "\n",
    "# Change permissions to 777 for files modified in the last 720 minutes (best-effort)\n",
    "_now = time.time()\n",
    "for path in outpath.rglob('*'):\n",
    "    try:\n",
    "        if path.is_file() and (_now - path.stat().st_mtime) <= 720 * 60:\n",
    "            os.chmod(path, 0o777)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"filtered detail rows: {len(filtered_detail)} | \"\n",
    "      f\"detail workbook: {_detail_xlsx.name} | pivot workbook: {_pivot_xlsx.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85eb97d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
